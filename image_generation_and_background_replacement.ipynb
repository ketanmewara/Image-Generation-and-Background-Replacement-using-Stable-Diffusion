{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMUrYbIw2qI9s0S3LFRDGJc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ketanmewara/Image-Generation-and-Background-Replacement-using-Stable-Diffusion/blob/main/image_generation_and_background_replacement.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Image Generation and Background Replacement using Stable Diffusion\n",
        "\n",
        "## Introduction\n",
        "\n",
        "This project demonstrates how to leverage various image generation models to create and modify images using Stable Diffusion. The notebook covers:\n",
        "- **Image Generation with Text-to-Image and Image-to-Image models.**\n",
        "- **Using Stable Diffusion 3.5 and 3 models for image manipulation.**\n",
        "- **Implementing object replacement and background modification through automatic masking.**\n",
        "- **Exploring the capabilities of model quantization for efficiency.**\n",
        "\n",
        "The notebook will guide you through model selection, image generation, and techniques to replace the background of images using AI models.\n",
        "\n",
        "## Goals\n",
        "- Explore the functionalities of different Stable Diffusion versions (3 and 3.5).\n",
        "- Implement a background replacement feature using image-to-image generation.\n",
        "- Apply adapter fusion (e.g., face/style adapters) for enhanced image generation.\n",
        "- Demonstrate automatic mask generation for object replacement.\n"
      ],
      "metadata": {
        "id": "c2E2o7aDm3QI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Selection\n",
        "\n",
        "#### Stable Diffusion Models Overview\n",
        "\n",
        "- **Text-to-Image:** Generates images from textual descriptions.\n",
        "- **Image-to-Image:** Modifies an image based on an input image with guidance from a text prompt.\n",
        "- **Inpainting:** Allows for specific parts of an image to be modified (e.g., replace background).\n",
        "\n",
        "### Key Differences:\n",
        "- Flux and Efficiency: Stable Diffusion 3.5 has Flux, which allows for smoother integration of text and images and faster results.\n",
        "- Image Quality: Stable Diffusion 3.5 generally offers higher quality and better fine-tuning options.\n",
        "- Speed and Resource Usage: Stable Diffusion 3.5 is typically more efficient, allowing for faster image generation with fewer resources.\n",
        "\n",
        "  In short, Stable Diffusion 3.5 is a more advanced and optimized version of Stable Diffusion 3, with improvements in speed, quality, and feature set.\n",
        "\n",
        "## Adapter Fusion\n",
        "Adapters such as face or style adapters can enhance the capabilities of Stable Diffusion models. These adapters allow for specific adjustments to images, like improving facial details or adjusting the artistic style of the output.\n",
        "\n",
        "## Quantization\n",
        "Quantization is used to optimize models for faster and more efficient inference, especially on hardware with limited resources. For Stable Diffusion models, this process can be done using diffusers for quantized weights.\n",
        "\n",
        "## Object Replacement Feature\n",
        "  1. Load the quantized model.\n",
        "  2. Automatically generate a mask.\n",
        "  3. Replace the object with a new one.\n",
        "  4. Display the results."
      ],
      "metadata": {
        "id": "0UHdNKFEvTi2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load the quantized model."
      ],
      "metadata": {
        "id": "OuiGrtbvlCqc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch torchvision supervision diffusers transformers bitsandbytes pillow"
      ],
      "metadata": {
        "id": "kQAy-nPhmoP7",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from diffusers import StableDiffusionInpaintPipeline, StableDiffusionPipeline\n",
        "from PIL import Image\n",
        "from transformers import BitsAndBytesConfig"
      ],
      "metadata": {
        "id": "09Wm_99SlKss"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(DEVICE)"
      ],
      "metadata": {
        "id": "xxTBVvi3lryT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nf4_config = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_quant_type=\"nf4\")"
      ],
      "metadata": {
        "id": "53tqHPhiBPj4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pipe = StableDiffusionInpaintPipeline.from_pretrained(\n",
        "    \"stabilityai/stable-diffusion-2-1\",\n",
        "    quantization_config=nf4_config, # quantization config\n",
        "    #torch_dtype=torch.float16,  # Use float16 for better performance\n",
        ").to(DEVICE)  # Move model to GPU if available"
      ],
      "metadata": {
        "id": "VProVFx_lZ28"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Automatically generate a mask"
      ],
      "metadata": {
        "id": "93TgskoPk-99"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6yqEnYnymtq6"
      },
      "outputs": [],
      "source": [
        "# Cloning the GitHub repository for Segment Anything model from Facebook Research\n",
        "!git clone \"https://github.com/facebookresearch/segment-anything\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Downloading a pre-trained model file (weights) from the provided URL\n",
        "!wget -q https://dl.fbaipublicfiles.com/segment_anything/sam_vit_b_01ec64.pth"
      ],
      "metadata": {
        "id": "rdvqY7Q_g0a3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing the necessary library for image handling\n",
        "from PIL import Image\n",
        "from io import BytesIO\n",
        "import requests\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "# Defining the URL of an image to process\n",
        "image_path = \"https://img.freepik.com/free-photo/adorable-cat-lifestyle_23-2151593320.jpg\"\n",
        "\n",
        "# Fetching the image from the URL and reading it into memory as bytes\n",
        "image_bytes = BytesIO(requests.get(image_path).content)\n",
        "\n",
        "# Opening the image using PIL from the downloaded bytes\n",
        "image = Image.open(image_bytes)\n",
        "\n",
        "# Converting the image into a NumPy array, then back to an image object\n",
        "image = Image.fromarray(np.array(image))\n",
        "\n",
        "# Resizing the image to a fixed 512x512 resolution\n",
        "image = image.resize((512, 512))"
      ],
      "metadata": {
        "id": "appz4QR6g0Xn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_type = \"vit_b\"\n",
        "model_path = '/content/sam_vit_b_01ec64.pth'\n",
        "print(model_path, \"exist:\", os.path.isfile(model_path))"
      ],
      "metadata": {
        "id": "1K6thLdTpUO5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.append('/content/segment-anything')"
      ],
      "metadata": {
        "id": "wWi8_dXepnA5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing necessary components from the Segment Anything module\n",
        "from segment_anything import sam_model_registry, SamAutomaticMaskGenerator, SamPredictor\n",
        "\n",
        "# Initializing the model using a checkpoint (weights) and moving it to the desired device (GPU/CPU)\n",
        "sam = sam_model_registry[model_type](checkpoint=model_path).to(device=DEVICE)\n",
        "\n",
        "# Creating a mask generator object from the initialized model\n",
        "mask_generator = SamAutomaticMaskGenerator(sam)\n",
        "\n",
        "# Generating the segmentation results from the image (as a NumPy array)\n",
        "result = mask_generator.generate(np.array(image))\n",
        "\n",
        "# Sorting the result by area and extracting segmentation data, bounding box information, point coordinates\n",
        "segmentations = [segment['segmentation'] for segment in sorted(result, key=lambda x: x['area'], reverse=True)]\n",
        "bboxs = [bbox['bbox'] for bbox in sorted(result, key=lambda x: x['area'], reverse=True)]\n",
        "points = [point['point_coords'] for point in sorted(result, key=lambda x: x['area'], reverse=True)]\n",
        "\n",
        "print(bboxs)\n",
        "print(points)\n",
        "# Checking the number of segmentations (masks) generated\n",
        "len(segmentations)"
      ],
      "metadata": {
        "id": "Up--hs2Gg0Uv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing a library for visualization\n",
        "import supervision as  sv\n",
        "\n",
        "# Displaying the first 10 segmentations as images in a grid of size (2 rows, 5 columns)\n",
        "sv.plot_images_grid(\n",
        "    images=segmentations[0:10],\n",
        "    grid_size=(2,5)\n",
        ")"
      ],
      "metadata": {
        "id": "39HvHCSxg0R4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "# Inverting the segmentation mask at index 1 in the segmentations list\n",
        "# The segmentation masks are binary, where `True` represents the object and `False` represents the background.\n",
        "# np.logical_not() inverts the boolean values, so `True` becomes `False` and `False` becomes `True`.\n",
        "mask = np.logical_not(segmentations[0])\n",
        "mask = Image.fromarray(mask).convert('L')"
      ],
      "metadata": {
        "id": "ewRNZ6Uuju5N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mask"
      ],
      "metadata": {
        "id": "EYJ3QvQcjuuo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Replace the object with a new one"
      ],
      "metadata": {
        "id": "WOmxd1zAlOE7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def inpaint_with_mask(image, mask, prompt):\n",
        "  image = Image.fromarray(np.array(image))\n",
        "  mask = Image.fromarray(np.array(mask))\n",
        "\n",
        "  image = image.resize((512, 512))\n",
        "  mask = mask.resize((512, 512))\n",
        "\n",
        "  inpainted_image = pipe(prompt=prompt, image=image, mask_image=mask).images[0]\n",
        "\n",
        "  return inpainted_image"
      ],
      "metadata": {
        "id": "gKMVUTC0lRW-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"background beach\"\n",
        "background_change_image = inpaint_with_mask(image, mask, prompt)"
      ],
      "metadata": {
        "id": "sBAaMiR3lRUF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Display the results."
      ],
      "metadata": {
        "id": "Me4gwj_kmUbH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the original image, mask image, and background replacement image\n",
        "import matplotlib.pyplot as plt\n",
        "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
        "\n",
        "# Display the images\n",
        "axes[0].imshow(image)\n",
        "axes[0].set_title('Original Image')\n",
        "axes[0].axis('off')\n",
        "\n",
        "axes[1].imshow(mask, cmap='gray')\n",
        "axes[1].set_title('Mask Image')\n",
        "axes[1].axis('off')\n",
        "\n",
        "axes[2].imshow(background_change_image)\n",
        "axes[2].set_title('After Background Replacement Image')\n",
        "axes[2].axis('off')\n",
        "\n",
        "# Show the plot\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "R1IpfFoLlRLs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GRxceTBalRAM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}